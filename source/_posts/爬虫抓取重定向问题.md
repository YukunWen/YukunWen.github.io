---
title: '"爬虫抓取重定向问题"' 
categories: java 
tags: java 
date: 2018-10-20 10:24:29 
---

在做爬虫中，我们经常会遇到**重定向**问题。比如之前做的秒拍的地址：

> [http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.htm](http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.htm)

在重定向后就成为了：

> [http://n.miaopai.com/media/fnrDABcheGpX2nrnpts7J9KC5ZlnVZ~Z](http://n.miaopai.com/media/fnrDABcheGpX2nrnpts7J9KC5ZlnVZ~Z)

从浏览器的network也可以看出来发生了一个重定向

![图1](http://upload-images.jianshu.io/upload_images/14043408-71520da4af6e9d52?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 所以如果我们直接请求原地址，并且用页面解析方法（如jsoup等），就无法获取到相应的视频标题，视频源码地址等信息。
<!--more-->
那么，就需要我们在该地址之前做一层**302重定向**到新的地址，根据新的地址就可以进行原有的解析工作。

1. 网上查到有一种简单的方法：  

``` java

        String url = "http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.htm";
        System.out.println("访问地址:" + url);
        URL serverUrl = new URL(url);
        HttpURLConnection conn = (HttpURLConnection) serverUrl.openConnection();
        conn.setRequestMethod("GET");
        conn.setRequestProperty("accept", "*/*");
        conn.setRequestProperty("connection", "Keep-Alive");
        // 必须设置false，否则会自动redirect到Location的地址
        conn.setInstanceFollowRedirects(false);
        //获取Location地址
        String location = conn.getHeaderField("Location");
        System.out.println(location);
```
该方法在本地进行模拟还可以，但是不适用于在生产上。
实际生产中爬虫都是高频次的爬取，所以任何的访问都必须带上代理才行。
结合公司目前采用的是httpClient框架，于是给出如下demo代码：  
2. 含有代理的httpClient实现
``` java
public static List<URI> doGetRedirectLocations(HttpGet httpReq){
        setHeaders(httpReq);
        httpReq.setConfig(instance.reqConfig);
        HttpClientContext localContext = getProxyContext();

        try (CloseableHttpResponse httpResp = instance.httpClient.execute(httpReq, localContext)){
            return localContext.getRedirectLocations();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }


public static HttpClientContext getProxyContext() {
        AuthCache authCache = new BasicAuthCache();
        authCache.put(instance.proxy, new BasicScheme());

        HttpClientContext localContext = HttpClientContext.create();
        localContext.setAuthCache(authCache);
        return localContext;
    }
```
更进一步地，由于大型网站都会有防爬虫的机制，即便你采用了动态IP的代理，但是还是难免在抓取的过程中可能会弹出需要输入验证码，或者进行滑块滑动验证的时候。
于是，更进一步的代码如下：
3. 增加轮询的视频抓取
``` java 
   public static List<URI> doGetRedirectLocations(HttpGet httpReq, int retry) {
        if (retry == 0) {
            log.error("重试次数已经用完" + httpReq);
            return null;
        }
        HttpClientContext localContext = getProxyContext();
        try (CloseableHttpResponse httpResp = instance.httpClient.execute(httpReq, localContext)) {
            List<URI> redirectLocations = localContext.getRedirectLocations();
            if (redirectLocations != null && redirectLocations.size() > 0) {
                return redirectLocations;
            } else {
                Thread.sleep(1000);
                return doGetRedirectLocations(httpReq, retry - 1);
            }
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
```
PS: 轮询有多重实现方式，初级工程师基本会使用for循环，那样的代码有时候冗余，可读性差。轮询最好用递归的方式，显得优雅。

至此，关于视频重定向的问题就这么解决了。最后顺口提及一句，在做爬虫解析过程中，我们发现了很多时候URL的尾部是非常不重要的。比如，本文章中的秒拍地址：

> [http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.htm](http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.htm)

你就算是写成

> [http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.html](http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__.html)

或者

> [http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__](http://www.miaopai.com/show/5bpIqSpDFRAvlgidhXSTKZkfSwWYpwaV5SI1ZA__)

一样可以无差别访问，因此在做正则匹配的时候，切记考虑通配尾部。

